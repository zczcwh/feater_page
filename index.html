
<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<script src="bootstrap.js"></script>
<script type="text/javascript" charset="utf-8" src="https://ajax.googleapis.com/ajax/libs/jquery/1.3.2/jquery.min.js"></script> 
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

<style type="text/css">
body {
    font-family: "Titillium Web", "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
    font-weight: 300;
    font-size: 17px;
    margin-left: auto;
    margin-right: auto;
}

@media screen and (min-width: 980px){
    body {
        width: 980px;
    }
}

h1 {
    font-weight:300;
    line-height: 1.15em;
}

h2 {
    font-size: 1.75em;
}
a:link,a:visited {
    color: #5364cc;
    text-decoration: none;
}
a:hover {
    color: #208799;
}
h1 {
    text-align: center;
}
h2,h3 {
    text-align: left;
}

h1 {
    font-size: 40px;
    font-weight: 500;
}
h2 {
    font-weight: 400;
    margin: 16px 0px 4px 0px;
}
h3 {
    font-weight: 600;
    margin: 16px 0px 4px 0px;
}

.paper-title {
    padding: 1px 0px 1px 0px;
}
section {
    margin: 32px 0px 32px 0px;
    text-align: justify;
    clear: both;
}
.col-5 {
     width: 20%;
     float: left;
}
.col-4 {
     width: 25%;
     float: left;
}
.col-3 {
     width: 33%;
     float: left;
}
.col-2 {
     width: 50%;
     float: left;
}
.col-1 {
     width: 100%;
     float: left;
}

.author-row, .affil-row {
    font-size: 26px;
}

.author-row-new { 
    text-align: center; 
}

.author-row-new a {
    display: inline-block;
    font-size: 20px;
    padding: 4px;
}

.author-row-new sup {
    color: #313436;
    font-size: 12px;
}

.affiliations-new {
    font-size: 18px;
    text-align: center;
    width: 80%;
    margin: 0 auto;
    margin-bottom: 20px;
}

.row {
    margin: 16px 0px 16px 0px;
}
.authors {
    font-size: 26px;
}
.affiliatons {
    font-size: 18px;
}
.affil-row {
    margin-top: 18px;
}
.teaser {
    max-width: 100%;
}
.text-center {
    text-align: center;  
}
.screenshot {
    width: 256px;
    border: 1px solid #ddd;
}
.screenshot-el {
    margin-bottom: 16px;
}
hr {
    height: 1px;
    border: 0; 
    border-top: 1px solid #ddd;
    margin: 0;
}
.material-icons {
    vertical-align: -6px;
}
p {
    line-height: 1.25em;
}
.caption {
    font-size: 16px;
    color: #666;
    margin-top: 4px;
    margin-bottom: 10px;
}


video {
    display: block;
    margin: auto;
}


figure {
    display: block;
    margin: auto;
    margin-top: 10px;
    margin-bottom: 10px;
}
#bibtex pre {
    font-size: 14px;
    background-color: #eee;
    padding: 16px;
}
.blue {
    color: #2c82c9;
    font-weight: bold;
}
.orange {
    color: #d35400;
    font-weight: bold;
}
.flex-row {
    display: flex;
    flex-flow: row wrap;
    padding: 0;
    margin: 0;
    list-style: none;
}

.paper-btn-coming-soon {
    position: relative; 
    top: 0;
    left: 0;
}

.coming-soon {
    position: absolute;
    top: -15px;
    right: -15px;
}

.paper-btn {
  position: relative;
  text-align: center;

  display: inline-block;
  margin: 8px;
  padding: 8px 8px;

  border-width: 0;
  outline: none;
  border-radius: 2px;
  
  background-color: #5364cc;
  color: white !important;
  font-size: 20px;
  width: 100px;
  font-weight: 600;
}
.paper-btn-parent {
    display: flex;
    justify-content: center;
    margin: 16px 0px;
}

.paper-btn:hover {
    opacity: 0.85;
}

.container {
    margin-left: auto;
    margin-right: auto;
    padding-left: 16px;
    padding-right: 16px;
}

.venue {
    font-size: 23px;
}

.topnav {
    background-color: #EEEEEE;
    overflow: hidden;
}

.topnav div {
    max-width: 1070px;
    margin: 0 auto;
}

.topnav a {
    display: inline-block;
    color: black;
    text-align: center;
    vertical-align: middle;
    padding: 16px 16px;
    text-decoration: none;
    font-size: 18px;
}

.topnav img {
    padding: 2px 0px;
    width: 100%;
    margin: 0.2em 0px 0.3em 0px;
    vertical-align: middle;
}

pre {
    font-size: 0.9em;
    padding-left: 7px;
    padding-right: 7px;
    padding-top: 3px;
    padding-bottom: 3px;
    border-radius: 3px;
    background-color: rgb(235, 235, 235);
    overflow-x: auto;
}

.download-thumb {
    display: flex;
}

@media only screen and (max-width: 620px) {
    .download-thumb {
        display: none;
    }
}

.paper-stuff {
    width: 50%;
    font-size: 20px;
}

@media only screen and (max-width: 620px) {
    .paper-stuff {
        width: 100%;
    }
}
* {
  box-sizing: border-box;
}

.column {
  text-align: center;
  float: left;
  width: 16.666%;
  padding: 5px;
}
.column3 {
  text-align: center;
  float: left;
  width: 33.333%;
  padding: 5px;
}
.column4 {
  text-align: center;
  float: left;
  width: 50%;
  padding: 5px;
}
.column5 {
  text-align: center;
  float: left;
  width: 20%;
  padding: 5px;
}
.border-right {
    border-right: 1px solid black;
}
.border-bottom{
    border-bottom: 1px solid black;
}



/* Clearfix (clear floats) */
.row::after {
  content: "";
  clear: both;
  display: table;
}
.img-fluid {
  max-width: 100%;
  height: auto;
}
.figure-img {
  margin-bottom: 0.5rem;
  line-height: 1;
}








.rounded-circle {
  border-radius: 50% !important;
}






/* Responsive layout - makes the three columns stack on top of each other instead of next to each other */
@media screen and (max-width: 500px) {
  .column {
    width: 100%;
  }
}
@media screen and (max-width: 500px) {
  .column3 {
    width: 100%;
  }
}

</style>
<link rel="stylesheet" href="bootstrap-grid.css">

<script type="text/javascript" src="../js/hidebib.js"></script>
    <link href='https://fonts.googleapis.com/css?family=Titillium+Web:400,600,400italic,600italic,300,300italic' rel='stylesheet' type='text/css'>
    <head>
        <title> FeatER: An Efficient Network for Human Reconstruction via Feature Map-Based TransformER</title>
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta property="og:description" content="FeatER: An Efficient Network for Human Reconstruction via Feature Map-Based TransformER"/>
        <link href="https://fonts.googleapis.com/css2?family=Material+Icons" rel="stylesheet">
        <meta name="twitter:title" content="FeatER: An Efficient Network for Human Reconstruction via Feature Map-Based TransformER">
        <meta name="twitter:description" content="">
        <meta name="twitter:image" content="">
    </head>

 <body>


<div class="container">
    <div class="paper-title">
    <h1> 
        FeatER: An Efficient Network for Human Reconstruction via Feature Map-Based TransformER
    </div>

    <div id="authors">
        <center>
            <div class="author-row-new">
                <a href="https://zczcwh.github.io/">Ce Zheng<sup>1</sup></a>,
                <a href="https://sites.google.com/view/matiasmendieta">Matias Mendieta<sup>1</sup></a>,
                <a href="https://taoyang1122.github.io/">Taojiannan Yang<sup>1</sup></a>,
                <a href="http://maple-lab.net/gqi/">Guo-Jun Qi<sup>2,3</sup></a>,
                <a href="https://www.crcv.ucf.edu/chenchen/">Chen Chen<sup>1</sup></a>,
            </div>
        </center>
        <center>
        <div class="affiliations">
            <span><sup>1</sup> Center for Research in Computer Vision, University of Central Florida </span><br/>
            <span><sup>2</sup> OPPO Seattle Research Center, USA </span>
            <span><sup>3</sup> Westlake University </span><br/>
        </div>

<!--         <br>*indicates equal contribution. -->

        <div class="affil-row">
            <div class="venue text-center"><b> CVPR 2023 </b></div>
        </div>

        </center>

        <div style="clear: both">
            <div class="paper-btn-parent">
            <a class="paper-btn" href="https://arxiv.org/pdf/2205.15448.pdf">
                <span class="material-icons"> description </span> 
                 Paper
            </a>
            <div class="paper-btn-coming-soon">
                <a class="paper-btn" href="https://github.com/zczcwh/FeatER">
                    <span class="material-icons"> code </span>
                    Code
                </a>
            <div class="paper-btn-coming-soon">
                <a class="paper-btn" href="https://www.youtube.com/watch?v=UErFBdDD7OI">
                    <span class="material-icons"> Video </span>
                    Video
                </a>             
            </div>
        </div></div></div>
    </div>

   
        
<!--     </section>
    <section id="teaser-image">
        <hr>
            <div class="mx-auto">
                <center><img class="card-img-top" src="materials/potter.gif" style="width:350px"></center>
            </div>
        <hr> -->
    
    

    
    <section id="abstract"/>
        <hr>
        <h2>Abstract</h2>
        <div class="flex-row">
            <p>
                Recently, vision transformers have shown great success in a set of human reconstruction tasks such as 2D/3D human pose estimation (2D/3D HPE) and human mesh reconstruction (HMR) tasks. 
                In these tasks, feature map representations of the human structural information are often extracted first from the image by a CNN (such as HRNet), and then further processed by transformer to predict the heatmaps 
                for HPE or HMR. However, existing transformer architectures are not able to process these feature map inputs directly, forcing an unnatural flattening of the location-sensitive human structural information. 
                Furthermore, much of the performance benefit in recent HPE and HMR methods has come at the cost of ever-increasing computation and memory needs. Therefore, to simultaneously address these problems, 
                we propose FeatER, a novel transformer design that preserves the inherent structure of feature map representations when modeling attention while reducing memory and computational costs. 
                Taking advantage of FeatER, we build an efficient network for a set of human reconstruction tasks including 2D HPE, 3D HPE, and HMR. A feature map reconstruction module is applied to improve the performance of the estimated human pose and mesh. 
                Extensive experiments demonstrate the effectiveness of FeatER on various human pose and mesh datasets. For instance, FeatER outperforms the SOTA method MeshGraphormer by requiring 5% of Params and 16% of MACs on Human3.6M and 3DPW datasets.        
        </div>
    </section>
    
    <section id="method"/>
        <hr>
        <h2>Overall framework</h2>

            <br><br>

            <div class="mx-auto">
                <center><img class="card-img-top" src="materials/framework_r2.jpg" style="width:950px"></center>
            </div>
        <hr>
    
        <h2>Our proposed FeatER blocks</h2>
            <br><br>
            <div class="mx-auto">
                <center><img class="card-img-top" src="materials/encoder.jpg" style="width:950px"></center>
            </div>
        <hr>
    
    
    </section>
        

    <section id="results">
        <hr>
        <h2>Results of image classification task </h2>
            <br><br>
            <div class="mx-auto">
                <center><img class="card-img-top" src="materials/table_img.jpg" style="width:950px"></center>
            </div>
        <hr>

        <h2>Results of human mesh recovery</h2>
            <br><br>
            <div class="mx-auto">
                <center><img class="card-img-top" src="materials/table_hmr.jpg" style="width:950px"></center>
            </div>
        <hr>

        <h2>Mesh visualization</h2>
            <br><br>
            <div class="mx-auto">
                <center><p><b> Visualization of pose and mesh </b></p></center>
                <center><img class="card-img-top" src="materials/vis_2d3d.jpg" style="width:100%"></center>
            </div>
            <br><br>
            <div class="mx-auto">
                <center><p><b>More qualitative results</b></p></center>
                <center><img class="card-img-top" src="materials/supp_vis_esay.jpg" style="width:100%"></center>
                <center><img class="card-img-top" src="materials/supp_vis_hard.jpg" style="width:100%"></center>
            </div>
            <br><br>
            <div class="mx-auto">
                <center><p><b>Qualitative comparison with SOTA methods</b></p></center>
                <center><img class="card-img-top" src="materials/supp_vis_compare.jpg" style="width:100%"></center>
            </div>
            <br><br><br>
        <hr>

    </section>
   
    <section id="paper">
        <h2>Bibtex</h2>
        <div class="page-body"><pre id="ad6975be-3353-467d-ae48-6313d767ffa6" class="code"><code>
            @InProceedings{zheng2023feater,
                title={FeatER: An Efficient Network for Human Reconstruction via Feature Map-Based TransformER},
                author={Zheng, Ce and Mendieta, Matias and Yang, Taojiannan and Qi, Guo-Jun and Chen, Chen},
                booktitle ={IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
                year={2023}
            }
        </code></pre><p id="1a3aa306-c4b8-4872-8fb0-411495c73d55" class="">
        </p></div>

    </section>

<!-- 
    <section id="paper">
        <h2>Paper</h2>
        <hr>
        <div class="flex-row">
            <div class="download-thumb">
            <div style="box-sizing: border-box; padding: 16px; margin: auto;">
                <a href="https://energy-based-model.github.io/composing-pretrained-models/"><img class="screenshot" src="materials/thumb_finger.png"></a>
            </div>
        </div>
            <div class="paper-stuff">
                <p><b>Composing Ensembles of Pre-trained Models via Iterative Consensus</b></p>
                <p>Shuang Li, Yilun Du, Joshua B. Tenenbaum, Antonio Torralba, Igor Mordatch</p>
                <div><span class="material-icons"> description </span><a href="https://arxiv.org/abs/2210.06978"> arXiv version</a></div>
                <div><span class="material-icons"> integration_instructions </span><a href="https://github.com/nv-tlabs/LION"> Code</a></div>
            </div>
            </div>
        </div>
    </section>

 -->

    <!-- <section id="bibtex">
        <h2>Citation</h2>
        <hr>
        <pre><code>@inproceedings{zeng2022lion,
            title={LION: Latent Point Diffusion Models for 3D Shape Generation},
            author={Xiaohui Zeng and Arash Vahdat and Francis Williams and Zan Gojcic and Or Litany and Sanja Fidler and Karsten Kreis},
            booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
            year={2022}
        }</code></pre>
    </section> -->

    <section>
        This webpage template was adapted from <a href='https://nv-tlabs.github.io/LION/'>here</a>.
    </section>
    


</div>
</body>
</html>
